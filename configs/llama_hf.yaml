Llama_config:
    name: 'LlamaHF'
    d_model: 512 #4096
    vocab_size: 8192 #32000
    n_layers: 8 #32
    max_seq_len: 512
    query_heads: 16 #32
    kv_heads: 8 #8
    dropout: 0.4
    hidden_dim: 2048 #4*d_model
    use_cache: False
dataset_config:
    train_data: './data/tinyshakespeare.txt'
    val_data:
    test_data:
    tokenizer_path: './data/tiny_shakespeare.model'
    context_len: 32 #128
    batch_size: 64
training_config:
    trainer_config:
        default_root_dir: './results/llama_hf'
        logger: CSVLogger # [False, CSVLogger]
        log_every_n_steps: 1
        max_epochs: 2
        #accelerator: 'gpu'
        #devices: 2
    fit_config:
        ckpt_path:
    optimizer_config:
        lr: 5.0e-5
        weight_decay: 0.1
        warmup_duration: 20
