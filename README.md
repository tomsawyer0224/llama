This is a personal project, for educational purpose only!

Based on: \
  https://arxiv.org/abs/2302.13971 \
  https://arxiv.org/abs/2307.09288 \
  https://arxiv.org/abs/2108.12409 \
  https://github.com/Meta-Llama/llama \
  https://github.com/lucidrains/rotary-embedding-torch \
  https://huggingface.co/docs/transformers/main/en/model_doc/llama

About this project:
  1. Llama is a family of LLMs developed by Meta with some replacements: \
     LayerNorm >> RMSNorm \
     Positional Encoding >> Rotary Position Embedding \
     Multihead Attention >> Grouped Query Attention (with KV cache) \
     ReLU >> SwiGLU

  2. 
     
  
